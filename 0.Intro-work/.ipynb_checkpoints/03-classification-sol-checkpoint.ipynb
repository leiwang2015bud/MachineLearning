{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### COMP4670/8600 - Introduction to Statistical Machine Learning - Tutorial 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "If there are our training set, m subjects and n features for each subject: \n",
    "${(x^{(1)}, y^{(1)}),((x^{(2)}, y^{(2)}), ..., ((x^{(m)}, y^{(m)})}$\n",
    "where $y \\in \\{0,1\\}$\n",
    "This is our feature matrix: \n",
    "\n",
    "$ x =\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_0& x^{(2)}_0 & ... & x^{(m)}_0\\\\ \n",
    "x^{(1)}_1& x^{(2)}_1 & ... & x^{(m)}_1\\\\ \n",
    "x^{(1)}_2& x^{(2)}_2 & ... & x^{(m)}_2\\\\\n",
    "...\\\\ \n",
    "x^{(1)}_n& x^{(2)}_n & ... & x^{(m)}_n\\\\ \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1& 1 & ... & 1\\\\ \n",
    "x^{(1)}_1& x^{(2)}_1 & ... & x^{(m)}_1\\\\ \n",
    "x^{(1)}_2& x^{(2)}_2 & ... & x^{(m)}_2\\\\\n",
    "...\\\\ \n",
    "x^{(1)}_n& x^{(2)}_n & ... & x^{(m)}_n\\\\ \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_0\\\\ \n",
    "x_1\\\\ \n",
    "x_2\\\\\n",
    "...\\\\ \n",
    "x_n\\\\ \n",
    "\\end{bmatrix}$,\n",
    "where ( $x^j_0 = 1,  j \\in [1,m] , x \\in \\RR^{ (n+1) \\times m}$ )\n",
    "\n",
    "Thus, parameter matrix:\n",
    "\n",
    "$\\theta = \n",
    "\\begin{bmatrix}\n",
    "\\theta_0\\\\ \n",
    "\\theta_1\\\\ \n",
    "\\theta_2\\\\\n",
    "...\\\\ \n",
    "\\theta_n\\\\ \n",
    "\\end{bmatrix}\n",
    "\\theta \\in \\RR^{ (n+1) \\times 1}\n",
    "$, and  $ z = \\theta^Tx = \n",
    "\\begin{bmatrix}\n",
    "\\theta_0& \n",
    "\\theta_1& \n",
    "\\theta_2&\n",
    "...& \n",
    "\\theta_n \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_0\\\\ \n",
    "x_1\\\\ \n",
    "x_2\\\\\n",
    "...\\\\ \n",
    "x_n\\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Therefore, $y_{predict} = h(\\theta) = \\frac{1}{1+e^{-z}} = \n",
    "\\begin{bmatrix}\n",
    "y_1\\\\ \n",
    "y_2\\\\ \n",
    "y_3\\\\\n",
    "...\\\\ \n",
    "y_m\\\\ \n",
    "\\end{bmatrix}, y \\in \\RR^{ m \\times 1}$\n",
    "\n",
    "We define that \n",
    "$cost(h_\\theta(x), y) = -(y)\\log(h_\\theta(x)) - (1-y)\\log(1-h_\\theta(x))$\n",
    "\n",
    ", so our cost function is $J(\\theta) = \\frac{1}{m}\\sum_{j = 1}^{m}cost(h_\\theta(x^{(j)}), y^{(j)})$.\n",
    "\n",
    "In order to minimize cost function $J(\\theta)$, we also need the gradient of the cost function as\n",
    "\n",
    "$\\tfrac{\\partial }{\\partial \\theta_i}J(\\theta) = \\sum_{j = 1}^{m}(h(\\theta^{(j)} - y^{(j)})x^{(j)}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\trace}[1]{\\operatorname{tr}\\left\\{#1\\right\\}}$\n",
    "$\\newcommand{\\Norm}[1]{\\lVert#1\\rVert}$\n",
    "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
    "$\\newcommand{\\inner}[2]{\\langle #1, #2 \\rangle}$\n",
    "$\\newcommand{\\DD}{\\mathscr{D}}$\n",
    "$\\newcommand{\\grad}[1]{\\operatorname{grad}#1}$\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\,min}$\n",
    "\n",
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize as opt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diabetes</th>\n",
       "      <th>num preg</th>\n",
       "      <th>plasma</th>\n",
       "      <th>bp</th>\n",
       "      <th>skin fold</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 0</td>\n",
       "      <td>-0.294118</td>\n",
       "      <td> 0.487437</td>\n",
       "      <td> 0.180328</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td> 0.001490</td>\n",
       "      <td>-0.531170</td>\n",
       "      <td>-0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 1</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-0.145729</td>\n",
       "      <td> 0.081967</td>\n",
       "      <td>-0.414141</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.207153</td>\n",
       "      <td>-0.766866</td>\n",
       "      <td>-0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 0</td>\n",
       "      <td>-0.058824</td>\n",
       "      <td> 0.839196</td>\n",
       "      <td> 0.049180</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.305514</td>\n",
       "      <td>-0.492741</td>\n",
       "      <td>-0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 1</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-0.105528</td>\n",
       "      <td> 0.081967</td>\n",
       "      <td>-0.535354</td>\n",
       "      <td>-0.777778</td>\n",
       "      <td>-0.162444</td>\n",
       "      <td>-0.923997</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td> 0.376884</td>\n",
       "      <td>-0.344262</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.602837</td>\n",
       "      <td> 0.284650</td>\n",
       "      <td> 0.887276</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diabetes  num preg    plasma        bp  skin fold   insulin       bmi  \\\n",
       "0         0 -0.294118  0.487437  0.180328  -0.292929 -1.000000  0.001490   \n",
       "1         1 -0.882353 -0.145729  0.081967  -0.414141 -1.000000 -0.207153   \n",
       "2         0 -0.058824  0.839196  0.049180  -1.000000 -1.000000 -0.305514   \n",
       "3         1 -0.882353 -0.105528  0.081967  -0.535354 -0.777778 -0.162444   \n",
       "4         0 -1.000000  0.376884 -0.344262  -0.292929 -0.602837  0.284650   \n",
       "\n",
       "   pedigree       age  \n",
       "0 -0.531170 -0.033333  \n",
       "1 -0.766866 -0.666667  \n",
       "2 -0.492741 -0.633333  \n",
       "3 -0.923997 -1.000000  \n",
       "4  0.887276 -0.600000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution\n",
    "names = ['diabetes', 'num preg', 'plasma', 'bp', 'skin fold', 'insulin', 'bmi', 'pedigree', 'age']\n",
    "data = pd.read_csv('diabetes_scale.csv', header=None, names=names)\n",
    "data.diabetes.replace(-1, 0, inplace=True) # replace -1 with 0 because we need labels to be in {0, 1}\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set\n",
    "\n",
    "We will predict the incidence of diabetes based on various measurements (see [description](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)). Instead of directly using the raw data, we use a normalised version, where the label to be predicted (the incidence of diabetes) is in the first column. Download the data from [mldata.org](http://mldata.org/repository/data/download/csv/diabetes_scale/).\n",
    "\n",
    "Read in the data using pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification via Logistic Regression\n",
    "\n",
    "Implement binary classification using logistic regression for a data set with two classes. Make sure you use appropriate [python style](https://www.python.org/dev/peps/pep-0008/) and [docstrings](https://www.python.org/dev/peps/pep-0257/).\n",
    "\n",
    "Use ```scipy.optimize.fmin_bfgs``` to optimise your cost function. ```fmin_bfgs``` requires the cost function to be optimised, and the gradient of this cost function. Implement these two functions as ```cost``` and ```grad``` by following the equations in the lectures.\n",
    "\n",
    "Implement the function ```train``` that takes a matrix of examples, and a vector of labels, and returns the maximum likelihood weight vector for logistic regresssion. Also implement a function ```test``` that takes this maximum likelihood weight vector and the a matrix of examples, and returns the predictions. See the section **Putting everything together** below for expected usage.\n",
    "\n",
    "We add an extra column of ones to represent the constant basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diabetes</th>\n",
       "      <th>num preg</th>\n",
       "      <th>plasma</th>\n",
       "      <th>bp</th>\n",
       "      <th>skin fold</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>ones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 0</td>\n",
       "      <td>-0.294118</td>\n",
       "      <td> 0.487437</td>\n",
       "      <td> 0.180328</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td> 0.001490</td>\n",
       "      <td>-0.531170</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 1</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-0.145729</td>\n",
       "      <td> 0.081967</td>\n",
       "      <td>-0.414141</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.207153</td>\n",
       "      <td>-0.766866</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 0</td>\n",
       "      <td>-0.058824</td>\n",
       "      <td> 0.839196</td>\n",
       "      <td> 0.049180</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.305514</td>\n",
       "      <td>-0.492741</td>\n",
       "      <td>-0.633333</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 1</td>\n",
       "      <td>-0.882353</td>\n",
       "      <td>-0.105528</td>\n",
       "      <td> 0.081967</td>\n",
       "      <td>-0.535354</td>\n",
       "      <td>-0.777778</td>\n",
       "      <td>-0.162444</td>\n",
       "      <td>-0.923997</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td> 0.376884</td>\n",
       "      <td>-0.344262</td>\n",
       "      <td>-0.292929</td>\n",
       "      <td>-0.602837</td>\n",
       "      <td> 0.284650</td>\n",
       "      <td> 0.887276</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diabetes  num preg    plasma        bp  skin fold   insulin       bmi  \\\n",
       "0         0 -0.294118  0.487437  0.180328  -0.292929 -1.000000  0.001490   \n",
       "1         1 -0.882353 -0.145729  0.081967  -0.414141 -1.000000 -0.207153   \n",
       "2         0 -0.058824  0.839196  0.049180  -1.000000 -1.000000 -0.305514   \n",
       "3         1 -0.882353 -0.105528  0.081967  -0.535354 -0.777778 -0.162444   \n",
       "4         0 -1.000000  0.376884 -0.344262  -0.292929 -0.602837  0.284650   \n",
       "\n",
       "   pedigree       age  ones  \n",
       "0 -0.531170 -0.033333     1  \n",
       "1 -0.766866 -0.666667     1  \n",
       "2 -0.492741 -0.633333     1  \n",
       "3 -0.923997 -1.000000     1  \n",
       "4  0.887276 -0.600000     1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ones'] = np.ones((data.shape[0], 1)) # add a column of ones\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are our training set, m subjects and n features for each subject: \n",
    "${(x^{(1)}, y^{(1)}),((x^{(2)}, y^{(2)}), ..., ((x^{(m)}, y^{(m)})}$\n",
    "where $y \\in \\{0,1\\}$\n",
    "\n",
    "However, our data set is in different form from above. It looks like,\n",
    "\n",
    "This is our feature matrix: \n",
    "\n",
    "$ x =\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_0& x^{(1)}_1 & ... & x^{(1)}_n\\\\ \n",
    "x^{(2)}_0& x^{(2)}_1 & ... & x^{(2)}_n\\\\ \n",
    "x^{(3)}_0& x^{(3)}_1 & ... & x^{(3)}_n\\\\\n",
    "...\\\\ \n",
    "x^{(m)}_0& x^{(m)}_1 & ... & x^{(m)}_n\\\\ \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1& x^{(1)}_1 & ... & x^{(1)}_n\\\\ \n",
    "1& x^{(2)}_1 & ... & x^{(2)}_n\\\\ \n",
    "1& x^{(3)}_1 & ... & x^{(3)}_n\\\\\n",
    "...\\\\ \n",
    "1& x^{(m)}_1 & ... & x^{(m)}_n\\\\ \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_0& \n",
    "x_1& \n",
    "x_2&\n",
    "...& \n",
    "x_n& \n",
    "\\end{bmatrix}$,\n",
    "where ( $x^j_0 = 1,  j \\in [1,m] $ and $ x \\in  \\RR^{m \\times (n+1) }$)\n",
    "\n",
    "Thus, parameter matrix:\n",
    "\n",
    "$\\theta = \n",
    "\\begin{bmatrix}\n",
    "\\theta_0\\\\ \n",
    "\\theta_1\\\\ \n",
    "\\theta_2\\\\\n",
    "...\\\\ \n",
    "\\theta_n\\\\ \n",
    "\\end{bmatrix}\n",
    "$ where $ \\theta \\in  \\RR^{n+1 \\times 1 }$\n",
    "$z = x\\theta  = \n",
    "\\begin{bmatrix}\n",
    "x_0& \n",
    "x_1& \n",
    "x_2&\n",
    "...& \n",
    "x_n& \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\theta_0\\\\ \n",
    "\\theta_1\\\\ \n",
    "\\theta_2\\\\\n",
    "...\\\\ \n",
    "\\theta_n\\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Therefore, $y_{predict} = h(\\theta) = \\frac{1}{1+e^{-z}} = \n",
    "\\begin{bmatrix}\n",
    "y_0\\\\ \n",
    "y_1\\\\ \n",
    "y_2\\\\\n",
    "...\\\\ \n",
    "y_m\\\\ \n",
    "\\end{bmatrix}$, where $ y \\in  \\RR^{m \\times 1} $\n",
    "\n",
    "We define that \n",
    "$cost(h_\\theta(x), y) = -(y)\\log(h_\\theta(x)) - (1-y)\\log(1-h_\\theta(x))$\n",
    "\n",
    ", so our cost function is $J(\\theta) = \\frac{1}{m}\\sum_{i = 1}^{m}cost(h_\\theta(x^{(i)}), y^{(i)})$.\n",
    "\n",
    "In order to minimize cost function $J(\\theta)$, we also need the gradient of the cost function as\n",
    "\n",
    "$\\tfrac{\\partial }{\\partial \\theta_j}J(\\theta) = \\sum_{i = 1}^{m}(h(\\theta^{(i)} - y^{(i)})x^{(i)}_j = (h_\\theta(x)-y)x_j $.\n",
    "$\\tfrac{\\partial }{\\partial \\theta}J(\\theta) = (h_\\theta(x)-y)x $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"S shaped function, known as the sigmoid\"\"\"\n",
    "    return 1 / (1 + np.exp(- Z))\n",
    "\n",
    "def cost(theta, X, y):\n",
    "    \"\"\"The cost function for logistic regression\"\"\"\n",
    "    p_1 = sigmoid(np.dot(X, theta)) # predicted probability of label 1\n",
    "    log_l = (-y)*np.log(p_1) - (1-y)*np.log(1-p_1) # log-likelihood vector\n",
    "\n",
    "    return log_l.mean()\n",
    "\n",
    "def grad(theta, X, y):\n",
    "    \"\"\"The gradient of the cost function for logistic regresssion\"\"\"\n",
    "    p_1 = sigmoid(np.dot(X, theta))\n",
    "    error = p_1 - y # difference between label and prediction\n",
    "    grad = np.dot(error, X) / y.size # gradient vector\n",
    "\n",
    "    return grad\n",
    "\n",
    "def train(X, y):\n",
    "    \"\"\"Train a logistic regression model for data X and labels y.\n",
    "    X = [1, x_1, x_2, ..., x_n] size: m x (n+1)\n",
    "    y = [y_1, y_2, y_3, ..., y_m].T size: m x 1\n",
    "    \n",
    "    returns the learned parameter.\n",
    "    theta = [w_0, w_1, w_2, ..., w_n].T size: (n+1) x 1\n",
    "    \n",
    "    \"\"\"\n",
    "    n = X.shape[1]# find the col number of matrix X\n",
    "    theta = 0.1*np.random.randn( n)\n",
    "    # we need to put five parameters into opt.fmin_bfgs function\n",
    "    # cost function, initial theta, gradient of cost function, feature matrix, target matrix\n",
    "    theta_best = opt.fmin_bfgs(cost, theta, fprime=grad, args=(X, y))\n",
    "    return theta_best\n",
    "\n",
    "def predict(theta_best, Xtest, threshold):\n",
    "    \"\"\"Using the learned parameter theta_best, predict on data Xtest\n",
    "        theta_best = [w_0, w_1, w_2, ..., w_n] size: 1 x (n+1)\n",
    "        Xtest = [1, x_1, x_2, ..., x_n] size: (n+1) x m\n",
    "        \n",
    "        z = theta X  size: 1 x m\n",
    "        h = g(z)\n",
    "    \"\"\"\n",
    "    h = sigmoid(theta_best.dot(Xtest.T))\n",
    "    \n",
    "    \"\"\"\n",
    "        Here we define that h is the predicted probability of y = 1.\n",
    "        Thus, if the h[1] = 60%, that means y_predict[1] = 1 is in 60% probability.\n",
    "        We could find the threshold = 50%,\n",
    "        say, if h[i] > 50%, we regard y_predict[i] as 1.\n",
    "    \"\"\"\n",
    "    for i in range(len(h)):\n",
    "        if h[i] > threshold:\n",
    "            h[i] = 1 # here we also can define a new matrix as y_predict\n",
    "        else:\n",
    "            h[i] = 0 # but we want to save space, so re-use matrix h\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance measure\n",
    "\n",
    "There are many ways to compute the [performance of a binary classifier](http://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers). The key concept is the idea of a confusion matrix or contingency table:\n",
    "\n",
    "|              |    | Label |    |\n",
    "|:-------------|:--:|:-----:|:--:|\n",
    "|              |    |  +1   | -1 |\n",
    "|**Prediction**| +1 |    TP | FP |\n",
    "|              | -1 |    FN | TN |\n",
    "\n",
    "where\n",
    "* TP - true positive\n",
    "* FP - false positive\n",
    "* FN - false negative\n",
    "* TN - true negative\n",
    "\n",
    "Implement three functions, the first one which returns the confusion matrix for comparing two lists (one set of predictions, and one set of labels). Then implement two functions that take the confusion matrix as input and returns the **accuracy** and **balanced accuracy** respectively. The [balanced accuracy](http://en.wikipedia.org/wiki/Accuracy_and_precision) is the average accuracy of each class.\n",
    "\n",
    "**accurace = $ \\frac{TP + TN }{ TP + FN + FP + TN }$**\n",
    "\n",
    "**balanced accuracy = $ 0.5  \\frac{ TP}{ TP + FN } + 0.5 \\frac {TN }{ FP + TN }$**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1]\n",
      " [1 2]]\n",
      "0.666666666667\n",
      "0.333333333333\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "def confusion_matrix(prediction, labels):\n",
    "    \"\"\"Returns the confusion matrix for a list of predictions and (correct) labels\n",
    "        prediction = [y_predict1, y_predict2, ..., y_predictm]\n",
    "        labels = [y_1, y_2, ..., y_m]\n",
    "        \n",
    "        reuturn a matrix\n",
    "        cmatrix = [[tp,fp],[tn,fn]] size: 2 x 2\n",
    "    \"\"\"\n",
    "    assert len(prediction) == len(labels) # make sure there are same example numbers\n",
    "    def f(pr, la):\n",
    "        n = 0\n",
    "        for i in range(len(prediction)):\n",
    "            if prediction[i] == pr and labels[i] == la:\n",
    "                n += 1\n",
    "        return n\n",
    "    return np.matrix([[f(1, 1), f(1, 0)], [f(0, 1), f(0, 0)]])\n",
    "\n",
    "def confusion_matrix_advanced(prediction, labels):\n",
    "    \"\"\"Returns the confusion matrix for a list of predictions and (correct) labels\"\"\"\n",
    "    assert len(prediction) == len(labels)\n",
    "    f = lambda p, l: len(list(filter(lambda x: x == (p, l), zip(prediction, labels))))\n",
    "    return np.matrix([[f(1, 1), f(1, 0)], [f(0, 1), f(0, 0)]])\n",
    "\n",
    "def accuracy(cmatrix):\n",
    "    \"\"\"Returns the accuracy of a confusion matrix\n",
    "        accuracy = correct prediction number / all prediction number\n",
    "    \"\"\"\n",
    "    tp, fp, fn, tn = cmatrix.flatten().tolist()[0]\n",
    "    return 1.0*(tp + tn) / (tp + fp + fn + tn)\n",
    "\n",
    "def balanced_accuracy(cmatrix):\n",
    "    \"\"\"Returns the balanced accuracy of a confusion matrix\n",
    "        \n",
    "    \"\"\"\n",
    "    tp, fp, fn, tn = cmatrix.flatten().tolist()[0]\n",
    "    return 1.0*tp / 2 / (tp + fn) + tn / 2 / (tn + fp)\n",
    "\n",
    "M = confusion_matrix([1,1,1,0,0,0],[1,1,0,1,0,0])\n",
    "print M\n",
    "print accuracy(M)\n",
    "print balanced_accuracy(M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Consider the following code, which trains on all the examples, and predicts on the training set. Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.470993\n",
      "         Iterations: 65\n",
      "         Function evaluations: 66\n",
      "         Gradient evaluations: 66\n",
      "[ 0.  1.  0.  1.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0.  0.  1.  1.  1.\n",
      "  1.  1.  1.  1.  0.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  1.  1.  1.\n",
      "  0.  1.  1.  0.  0.  0.  1.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  0.\n",
      "  0.  1.  0.  1.  0.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  0.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0.  1.  0.  1.  0.  1.\n",
      "  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  0.  0.  1.  1.  0.  0.  1.  1.  1.  1.  0.  1.  1.  1.  1.  0.\n",
      "  1.  1.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  0.  1.  1.  1.  0.  0.  0.  0.  1.  1.  1.  0.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  0.  1.  0.  0.  0.\n",
      "  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.\n",
      "  1.  1.  1.  1.  0.  0.  1.  0.  1.  1.  1.  0.  0.  1.  0.  0.  1.  1.\n",
      "  1.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  0.  1.  0.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  1.  1.  1.  1.  0.  0.  1.  0.\n",
      "  0.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  0.  1.  0.  1.  1.  0.  1.\n",
      "  1.  1.  1.  1.  0.  1.  1.  0.  1.  1.  1.  1.  0.  1.  1.  0.  1.  1.\n",
      "  0.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  0.  1.  0.  1.  1.  1.  0.\n",
      "  1.  1.  1.  0.  1.  1.  1.  1.  0.  1.  1.  0.  0.  1.  0.  0.  1.  1.\n",
      "  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  0.  1.  0.\n",
      "  0.  0.  1.  0.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  0.  1.  1.\n",
      "  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  0.  1.  1.  0.  1.\n",
      "  1.  1.  1.  0.  1.  1.  1.  1.  0.  0.  1.  1.  0.  0.  1.  1.  0.  1.\n",
      "  1.  0.  1.  0.  1.  1.  1.  1.  1.  1.  0.  0.  1.  0.  1.  1.  1.  1.\n",
      "  1.  1.  1.  0.  0.  1.  1.  1.  0.  1.  1.  1.  1.  0.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  0.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  0.  1.\n",
      "  1.  0.  1.  0.  1.  1.  1.  1.  1.  0.  1.  1.  0.  0.  1.  1.  1.  1.\n",
      "  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  1.  1.  1.  1.\n",
      "  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  0.  1.  1.  0.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  0.  1.  0.  1.  0.  1.  1.  1.\n",
      "  1.  0.  1.  1.  0.  1.  1.  1.  1.  0.  0.  1.  0.  1.  1.  1.  1.  0.\n",
      "  0.  1.  0.  1.  1.  1.  0.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.\n",
      "  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  0.  1.  0.  0.  0.  0.  1.  1.\n",
      "  0.  1.  1.  0.  0.  1.  1.  0.  1.  0.  0.  1.  1.  1.  1.  0.  1.  1.\n",
      "  1.  1.  1.  1.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  1.  0.  1.  1.\n",
      "  0.  0.  1.  1.  1.  1.  0.  1.  1.  1.  0.  1.  1.  0.  0.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.\n",
      "  1.  1.  0.  1.  1.  0.  0.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.  1.\n",
      "  1.  1.  1.  0.  1.  0.  1.  1.  1.  1.  1.  1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7825520833333334, 0.445]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data pre-processing\n",
    "y = data['diabetes']\n",
    "X = data[['num preg', 'plasma', 'bp', 'skin fold', 'insulin', 'bmi', 'pedigree', 'age', 'ones']]\n",
    "\n",
    "#Training\n",
    "theta_best = train(X, y)\n",
    "threshold = 0.5\n",
    "#Testing\n",
    "pred = predict(theta_best, X, threshold)\n",
    "print pred\n",
    "\n",
    "# Evaluation\n",
    "cmatrix = confusion_matrix(pred, y)\n",
    "[accuracy(cmatrix), balanced_accuracy(cmatrix)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher's discriminant\n",
    "\n",
    "In the lectures, you saw that the Fisher criterion\n",
    "$$\n",
    "J(w) = \\frac{w^T S_B w}{w^T S_W w}\n",
    "$$\n",
    "is maximum for Fisher's linear discriminant.\n",
    "\n",
    "Define $S_B$ and $S_W$ as in the lectures and prove this result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "See Bishop, page 189.\n",
    "\n",
    "$$ S_B = (\\vec{m}_2 - \\vec{m}_1)(\\vec{m}_2 - \\vec{m}_1)^T $$\n",
    "\n",
    "$$ S_W = \\sum_{i \\in \\{ 1, 2 \\}} \\sum_{n \\in \\mathcal{C}_i} (\\vec{x}_n - \\vec{m}_i)(\\vec{x}_n - \\vec{m}_i)^T $$\n",
    "\n",
    "Differentiating $J(w)$ with respect to $w$, we get\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w_i}(w) = \\frac{1}{(w^T S_W w)^2} ((w^T S_B e_i + e_i^T S_B w)(w^T S_W w) - (w^T S_W e_i + e_i^T S_W w) (w^T S_B w)). $$\n",
    "\n",
    "If this is $0$, we have a local extremum.\n",
    "This is implied by the following sufficient condition:\n",
    "\n",
    "$$ (e_i^T S_B w)(w^T S_W w) - (e_i^T S_W w)(w^T S_B w) = 0 $$\n",
    "\n",
    "Accumulating these conditions for every $i$ into one linear equation system:\n",
    "\n",
    "$$ S_B w (w^T S_W w) - S_W w (w^T S_B w) = 0 $$\n",
    "\n",
    "Since we are only interested in the direction of $w$, we can ignore the scalar factors $w^T S_W w$ and $w^T S_B w$ and\n",
    "Thus we introduce a new scalar $c$:\n",
    "\n",
    "$$ S_W w = c S_B w $$\n",
    "\n",
    "By definition of $S_B$,\n",
    "\n",
    "$$ S_W w = c (\\vec{m}_2 - \\vec{m}_1) (\\vec{m}_2 - \\vec{m}_1)^T w. $$\n",
    "\n",
    "Therefore $S_W w = c' (\\vec{m}_2 - \\vec{m}_1)$, for the new scalar $c' = c (\\vec{m}_2 - \\vec{m}_1)^T w$.\n",
    "\n",
    "We assume that $S_W$ is invertable and get the desired result:\n",
    "\n",
    "$$ w = c' S_W^{-1} (\\vec{m}_2 - \\vec{m}_1) $$\n",
    "\n",
    "We know this is a local extremum,\n",
    "it only remains to check that it is indeed a maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "See Bishop, page 189.\n",
    "\n",
    "$$ S_B = (\\vec{m}_2 - \\vec{m}_1)(\\vec{m}_2 - \\vec{m}_1)^T $$\n",
    "\n",
    "$$ S_W = \\sum_{i \\in \\{ 1, 2 \\}} \\sum_{n \\in \\mathcal{C}_i} (\\vec{x}_n - \\vec{m}_i)(\\vec{x}_n - \\vec{m}_i)^T $$\n",
    "\n",
    "It often helps to try to simplify a problem before we apply the mathematical machinery to it.\n",
    "Let's try it for this example.\n",
    "\n",
    "The most annoying part seems to be the denominator with the with-in class covariance. Starting from the original objective $ J(w) = \\frac{w^T S_B w}{w^T S_W w} $, we observe that\n",
    "in general the with-in class covariance is positive definite and therefore we can write the\n",
    "eigenvalue decomposition $ S_W = Q \\Lambda Q^T $ with eigenvector matrix $ Q $ and eigenvalues in the diagonal matrix $ \\Lambda $.\n",
    "\n",
    "We now introduce a new variable $ v $ which relates to $ w $ by $ v = \\Lambda^{1/2} Q^T w $.\n",
    "Using this new variable $ v $, we can rewrite our objective as\n",
    "$$\n",
    "J(v) = \\frac{v^T \\Lambda^{-1/2} Q^T S_B Q \\Lambda^{-1/2} v}{v^T v}.\n",
    "$$\n",
    "\n",
    "We can now insert the definition $ S_B = (\\vec{m}_2 - \\vec{m}_1)(\\vec{m}_2 - \\vec{m}_1)^T $ to get\n",
    "$$\n",
    "J(v) = \\frac{v^T \\Lambda^{-1/2} Q^T (\\vec{m}_2 - \\vec{m}_1)\n",
    "(\\vec{m}_2 - \\vec{m}_1)^T Q \\Lambda^{-1/2} v}{v^T v} =\n",
    "\\frac{\\left((\\vec{m}_2 - \\vec{m}_1)^T Q \\Lambda^{-1/2} v \\right)^2}{\\Vert v\\Vert^2}\n",
    "$$\n",
    "\n",
    "The scalar in the squared nominator is an inner product which is maximised if the vector $ (\\vec{m}_2 - \\vec{m}_1)^T Q \\Lambda^{-1/2} $ is equal to $ v^T $ or with other words\n",
    "$$\n",
    "v = \\Lambda^{-1/2} Q^T (\\vec{m}_2 - \\vec{m}_1).\n",
    "$$\n",
    "\n",
    "Finally, inverting the relation between $ w $ and $ v $ above to $ w = Q \\Lambda^{-1/2} v $ we\n",
    "get for the optimising $ w $\n",
    "$$\n",
    "w = Q \\Lambda^{-1} Q^T (\\vec{m}_2 - \\vec{m}_1) = S_W^{-1} (\\vec{m}_2 - \\vec{m}_1).\n",
    "$$\n",
    "where we have used the fact that given the eigendecomposition $ S_W = Q \\Lambda Q^T $, we\n",
    "get the eigendecomposition of the inverse by inverting the eigenvalues, $ S_W^{-1} = Q \\Lambda^{-1} Q^T $.\n",
    "\n",
    "Without gradient and Hessian calculation, we have shown that \n",
    "$$\n",
    "w = S_W^{-1} (\\vec{m}_2 - \\vec{m}_1)\n",
    "$$\n",
    "maximises the Fisher discriminant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Effect of regularization parameter\n",
    "\n",
    "By splitting the data into two halves, train on one half and report performance on the second half. By repeating this experiment for different values of the regularization parameter $\\lambda$ we can get a feeling about the variability in the performance of the classifier due to regularization. Plot the values of accuracy and balanced accuracy for at least 3 different choices of $\\lambda$. Note that you may have to update your implementation of logistic regression to include the regularisation parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Solution\n",
    "\n",
    "def split_data(data):\n",
    "    \"\"\"Randomly split data into two equal groups\"\"\"\n",
    "    np.random.seed(1)\n",
    "    N = len(data)\n",
    "    idx = np.arange(N)\n",
    "    np.random.shuffle(idx)\n",
    "    train_idx = idx[:int(N/2)]\n",
    "    test_idx = idx[int(N/2):]\n",
    "\n",
    "    X_train = data.loc[train_idx].drop('diabetes', axis=1)\n",
    "    t_train = data.loc[train_idx]['diabetes']\n",
    "    X_test = data.loc[test_idx].drop('diabetes', axis=1)\n",
    "    t_test = data.loc[test_idx]['diabetes']\n",
    "    \n",
    "    return X_train, t_train, X_test, t_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seperateRowDataSet(X,y):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    X: raw feature matrix, X \\in R^{m \\times n}\n",
    "    y: label column vector, y \\in R^{m \\times 1}\n",
    "    \n",
    "    Return\n",
    "    X_training, \\in R^{m/2 \\times n}\n",
    "    y_training, \\in R^{m/2 \\times 1}\n",
    "    X_testing, \\in R^{m/2 \\times n}\n",
    "    y_testing, \\in R^{m/2 \\times 1}\n",
    "    \"\"\"\n",
    "    X_class1,X_class2 = splitXintoTwoClass(X,y)\n",
    "    a = X_class1[0]\n",
    "    b = X_class2[0]    \n",
    "    inda = np.arrange(a)\n",
    "    indb = np.arrange(b)   \n",
    "    np.random.seed(11109)\n",
    "    np.random.shuffle(inda)\n",
    "    p.random.shuffle(indb)\n",
    "    ############### class 1 \n",
    "    train_idx_a = idxa[0:int(1+((a-1)/2))]\n",
    "    test_idx_a = idxa[int(1+((a-1)/2)):]\n",
    "   \n",
    "    X_training_a = X_class1[train_idx_a]\n",
    "    y_training_a = np.ones((((a-1)/2)+1),1))\n",
    "    \n",
    "    X_testing_a = X_class1[test_idx_a]\n",
    "    y_testing_a = np.ones((((a-1)/2),1))    \n",
    "    ################ class 2\n",
    "    test_idx_b = idxa[0:int(1+((b-1)/2))]\n",
    "    train_idx_b = idxa[int(1+((b-1)/2)):]\n",
    "   \n",
    "    X_testing_b = X_class1[test_idx_b]\n",
    "    y_testing_b = np.ones((((b-1)/2)+1),1))\n",
    "    \n",
    "    X_training_b = X_class1[train_idx_b]\n",
    "    y_training_b = np.ones((((b-1)/2),1))\n",
    "    ################ Combine together\n",
    "    X_training = np.append(X_training_a,X_training_b,axis = 0)\n",
    "    y_training = np.append(y_training_a,y_training_b,axis = 0)\n",
    "    \n",
    "    X_testing = np.append(X_testing_a,X_testing_b,axis = 0)\n",
    "    y_testing = np.append(y_testing_a,y_testing_b,axis = 0)\n",
    "       \n",
    "    return X_training,y_training,X_testing,y_testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
