{
 "metadata": {
  "name": "05-kernel-regression-work.ipynb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Kernel Methods"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "COMP4670/8600 - Introduction to Statistical Machine Learning - Tutorial 5"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\newcommand{\\RR}{\\mathbb{R}}$\n",
      "$\\newcommand{\\dotprod}[2]{\\langle #1, #2 \\rangle}$\n",
      "\n",
      "Setting up the environment"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The data set\n",
      "\n",
      "This is the same dataset we used in Tutorial 2.\n",
      "\n",
      "*We will use an old dataset on the price of housing in Boston (see [description](https://archive.ics.uci.edu/ml/datasets/Housing)). The aim is to predict the median value of the owner occupied homes from various other factors. We will use a normalised version of this data, where each row is an example. The median value of homes is given in the first column (the label) and the value of each subsequent feature has been normalised to be in the range $[-1,1]$. Download this dataset from [mldata.org](http://mldata.org/repository/data/download/csv/housing_scale/).*\n",
      "\n",
      "Read in the data using pandas. Remove the column containing the binary variable 'CHAS' using ```drop```, which should give you a DataFrame with 506 rows (examples) and 13 columns (1 label and 12 features)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names = ['medv', 'crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat']\n",
      "data = pd.read_csv('housing_scale.csv', header=None, names=names)\n",
      "data.head()\n",
      "data.drop('chas', axis=1, inplace=True)\n",
      "data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "(506, 13)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Constructing new kernels\n",
      "\n",
      "In the lectures, we saw that certain operations on kernels preserve positive semidefiniteness. Recall that a symmetric matrix $K\\in \\RR^n \\times\\RR^n$ is positive semidefinite if for all vectors $a\\in\\RR^n$ we have the inequality\n",
      "$$\n",
      "a^T K a \\geqslant 0.\n",
      "$$\n",
      "\n",
      "Prove the following relations:\n",
      "1. Given positive semidefinite matrices $K_1$, $K_2$, show that $K_1 + K_2$ is a valid kernel.\n",
      "2. Given a positive semidefinite matrix $K$, show that $K^2 = K\\cdot K$ is a valid kernel, where the multiplication is a pointwise multiplication (not matrix multiplication)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Solution description\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Polynomial kernel using closure\n",
      "\n",
      "Using the properties proven above, show that the inhomogenous polynomial kernel of degree 2\n",
      "$$k(x,y) = (\\dotprod{x}{y} + 1)^2$$\n",
      "is positive semidefinite."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Solution description\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Empirical comparison\n",
      "\n",
      "Recall from Tutorial 2 that we could explicitly construct the polynomial basis function. In fact this demonstrates the relation\n",
      "$$\n",
      "k(x,y) = (\\dotprod{x}{y} + 1)^2 = \\dotprod{\\phi(x)}{\\phi(y)}.\n",
      "$$\n",
      "where\n",
      "$$\n",
      "\\phi(x) = (x_1^2, x_2^2, \\ldots, x_n^2, \\sqrt{2}x_1 x_2, \\ldots, \\sqrt{2}x_{n-1} x_n, \\sqrt{2}x_1, \\ldots, \\sqrt{2}x_n, 1)\n",
      "$$\n",
      "*This is sometimes referred to as an explicit feature map or the primal version of a kernel method.*\n",
      "\n",
      "For the data above, construct two kernel matrices, one using the explicit feature map and the second using the equation for the polynomial kernel. Confirm that they are the same."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Solution goes here\n",
      "def split_data(data):\n",
      "    \"\"\"Randomly split data into two equal groups\"\"\"\n",
      "    np.random.seed(1)#It can be called again to re-seed the generator. \n",
      "    N = len(data)# find the rows number(subject number) for data matrix\n",
      "    idx = np.arange(N) # build N x 1 row narray,[0, 1, 2, ..., N]\n",
      "    np.random.shuffle(idx) # re-allocate position for every elements\n",
      "    train_idx = idx[:int(N/2)] # sub-split the first int(N/2) elements\n",
      "    test_idx = idx[int(N/2):] # sub-split the rest elements\n",
      "\n",
      "    X_train = data.loc[train_idx].drop('medv', axis=1)\n",
      "    # extract the elements = data[rows][cols]\n",
      "    t_train = data.loc[train_idx]['medv']\n",
      "    X_test = data.loc[test_idx].drop('medv', axis=1)\n",
      "    t_test = data.loc[test_idx]['medv']\n",
      "    \n",
      "    return X_train, t_train, X_test, t_test\n",
      "\n",
      "# Return an row vector\n",
      "def phi_quadratic(x):\n",
      "    \"\"\"Compute phi(x) for a single training example using \n",
      "    quadratic basis function.\"\"\"\n",
      "    D = len(x)\n",
      "    # Features are (1, {x_i}, {cross terms and squared terms})\n",
      "    # Cross terms x_i x_j and squared terms x_i^2 can be taken \n",
      "    # from upper triangle of outer product of x with itself\n",
      "    return np.hstack((1, x, np.outer(x, x)[np.triu_indices(D)]))\n",
      "\n",
      "X_train, t_train, X_test, t_test = split_data(data)\n",
      "Phi_train_X = np.array([phi_quadratic(x) for i,x in X_train.iterrows()])\n",
      "Phi_test_X = np.array([phi_quadratic(x) for i,x in X_test.iterrows()])\n",
      "Phi_train = Phi_train[:]\n",
      "train=(np.matrix(Phi_train_X)).T\n",
      "test=(np.matrix(Phi_test_X)).T\n",
      "right=train*test.T\n",
      "print right.shape\n",
      "right_array = dot(Phi_train_X.T,Phi_test_X)\n",
      "print right_array.shape\n",
      "\n",
      "inner = dot(X_train,X_test.T)\n",
      "# print X_train.shape\n",
      "# print X_test.shape\n",
      "left = dot(inner + eye(len(inner)),inner + eye(len(inner)))\n",
      "\n",
      "# print 'left',left \n",
      "# print 'right',right\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(253, 253)\n",
        "(91, 91)\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are pros and cons for each method of computing the kernel matrix. Discuss."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Solution description\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Regularized least squares with kernels\n",
      "\n",
      "This section is analogous to the part in Tutorial 2 about regularized least squares.\n",
      "\n",
      "State carefully the cost function and the regulariser, defining all symbols, show that the regularized least squares solution can be expressed as in Lecture 5 and Lecture 11.\n",
      "$$\n",
      "w = \\left( \\lambda \\mathbf{I} + \\Phi^T \\Phi\\right)^{-1} \\Phi t\n",
      "$$\n",
      "Please describe the reason for each step."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Solution description\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By substituting $w = \\Phi^T a$, derive the regularized least squares method in terms of the kernel matrix $K$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Solution description\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Comparing solutions in $a$ and $\\mathbf{w}$\n",
      "\n",
      "Implement the kernelized regularized least squares as above. \n",
      "*This is often referred to as the dual version of the kernel method.*\n",
      "\n",
      "Compare this with the solution from Tutorial 2. Implement two classes:\n",
      "* ```RLSPrimal```\n",
      "* ```RLSDual```\n",
      "\n",
      "each which contain a ```train``` and ```predict``` function.\n",
      "\n",
      "Think carefully about the interfaces to the training and test procedures for the two different versions of regularized least squares. Also think about the parameters that need to be stored in the class."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Solution goes here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## (optional) General kernel\n",
      "\n",
      "Consider how you would generalise the two classes above if you wanted to have a polynomial kernel of degree 3. For the primal version, assume you have a function that returns the explicit feature map for the kernel ```feature_map(X)``` and for the dual version assume you have a function that returns the kernel matrix ```kernel_matrix(X)```."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}